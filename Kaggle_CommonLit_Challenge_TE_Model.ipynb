{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle CommonLit Readability Challenge\n",
    "Python version: 3.7 \\\n",
    "Date: 24-Nov-2021 \\\n",
    "This code uses PyTorch's tranformer encoder for estimating the score on sentence(s) written in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "# from plotnine import *\n",
    "\n",
    "from torch import Tensor\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Kaggle_CommonLit_Challenge_TE_Func.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-batch transformer-encoder based Predictive Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters\n",
    "words_per_essay = 77\n",
    "\n",
    "vec_len_per_word=50\n",
    "# vec_len_per_word=100\n",
    "# vec_len_per_word=200\n",
    "# vec_len_per_word=300\n",
    "\n",
    "number_of_heads = 1\n",
    "number_of_layers = 1\n",
    "\n",
    "num_feed_fwd_dim=2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read the excerpt embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_embed = np.load('Embeddings_Valid_Excerpt.npy')\n",
    "# essay_embed = np.load('Embeddings_Valid_Excerpt_Dim_100.npy')\n",
    "# essay_embed = np.load('Embeddings_Valid_Excerpt_Dim_200.npy')\n",
    "# essay_embed = np.load('Embeddings_Valid_Excerpt_Dim_300.npy')\n",
    "essay_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_score_frame = pd.read_csv('Scores_Valid_Excerpt.csv')\n",
    "lit_score = torch.tensor(lit_score_frame['target'], dtype=torch.float) #dtype has to be float32 and not float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spread of the 'target' values\n",
    "lit_score_frame['target'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_essays = lit_score_frame.shape[0]\n",
    "torch_essay = torch.zeros((words_per_essay, num_essays, vec_len_per_word))\n",
    "\n",
    "for idx in range(num_essays):\n",
    "    start_idx = 0 + idx*words_per_essay\n",
    "    end_idx = start_idx + words_per_essay\n",
    "    torch_essay[:, idx, :] = torch.tensor(essay_embed[start_idx:end_idx, :])\n",
    "\n",
    "# number_of_words x number_of_sentences x word_vec\n",
    "torch_essay.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_batches = int(np.ceil(lit_score_frame.shape[0]/batch_size))\n",
    "print(num_batches)\n",
    "\n",
    "rng = np.random.default_rng(100)\n",
    "elements_per_batch = rng.choice(num_batches, lit_score_frame.shape[0], replace=True)\n",
    "# np.unique(elements_per_batch, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of unique integers\n",
    "unique_batches = list(np.unique(elements_per_batch))\n",
    "len(unique_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of sentences\n",
    "len(elements_per_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70:30 split between training and test set\n",
    "0.7 * 2146"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_count_per_batch = np.unique(elements_per_batch, return_counts=True)[1]\n",
    "np.min(np.where(idx_count_per_batch.cumsum() >= 1502)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trng_set_batches = list(set(range(0, 48)) & set(unique_batches))\n",
    "test_set_batches = list(set(range(48, 68)) & set(unique_batches))\n",
    "\n",
    "# Sort the batches in ascending order to faciliate debugging\n",
    "trng_set_batches.sort()\n",
    "test_set_batches.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "\n",
    "# lit_model = Transformer_Encoder(embed_size=vec_len_per_word\n",
    "#                                 , n_heads=number_of_heads\n",
    "#                                 , n_layers=number_of_layers\n",
    "#                                 , output_size=[30, 10, 1]) # embed_size = size of input vector\n",
    "\n",
    "# lit_model = Transformer_Encoder_Self_Decoder(embed_size=vec_len_per_word\n",
    "#                                 , n_heads=number_of_heads\n",
    "#                                 , n_layers=number_of_layers\n",
    "#                                 , output_size=[30, 10, 1]) # embed_size = size of input vector\n",
    "\n",
    "lit_model = Transformer_Encoder_Pos_Embed(embed_size=vec_len_per_word\n",
    "                                , n_heads=number_of_heads\n",
    "                                , n_layers=number_of_layers\n",
    "                                , output_size=[30, 10, 1] # embed_size = size of input vector\n",
    "                                , dim_feedfwd=num_feed_fwd_dim) \n",
    "\n",
    "lit_loss_function = nn.MSELoss()\n",
    "lit_optimizer = torch.optim.Adam(lit_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of model parameters\n",
    "vec_len_per_word = [50, 100, 200, 300]\n",
    "number_of_heads=1\n",
    "number_of_layers=1\n",
    "num_feed_fwd_dim = [2048, 1024, 512, 256]\n",
    "\n",
    "num_param_set = []\n",
    "\n",
    "for vec_len in vec_len_per_word:\n",
    "    for dim_len in num_feed_fwd_dim:\n",
    "        \n",
    "        lit_model = Transformer_Encoder_Pos_Embed(embed_size=vec_len\n",
    "                                , n_heads=number_of_heads\n",
    "                                , n_layers=number_of_layers\n",
    "                                , output_size=[30, 10, 1] # embed_size = size of input vector\n",
    "                                , dim_feedfwd=dim_len) \n",
    "\n",
    "        temp = sum(p.numel() for p in lit_model.parameters() if p.requires_grad)\n",
    "        num_param_set = num_param_set + [temp]\n",
    "\n",
    "num_param_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "# num_epochs = 1\n",
    "test_loss_per_epoch = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch%1 == 0:\n",
    "        print(epoch)\n",
    "    \n",
    "    lit_model.train()\n",
    "    for trng_batch_idx in trng_set_batches:\n",
    "        current_batch_idx = np.where(elements_per_batch == trng_batch_idx)[0]\n",
    "        current_fit = lit_model(torch_essay[:, current_batch_idx, :])\n",
    "        current_loss = lit_loss_function(current_fit, torch.unsqueeze(lit_score[current_batch_idx], 1))  # 2\n",
    "        lit_optimizer.zero_grad()     # 3\n",
    "        current_loss.backward()       # 4\n",
    "        lit_optimizer.step()          # 5\n",
    "    \n",
    "    lit_model.eval()\n",
    "    with torch.no_grad():\n",
    "        temp_loss_per_batch = 0\n",
    "        for test_batch_idx in test_set_batches:\n",
    "            current_batch_idx = np.where(elements_per_batch == test_batch_idx)[0]\n",
    "            current_fit = lit_model(torch_essay[:, current_batch_idx, :])\n",
    "            current_loss = lit_loss_function(current_fit, torch.unsqueeze(lit_score[current_batch_idx], 1))  # 2\n",
    "            temp_loss_per_batch += current_loss.item()\n",
    "            # print(temp_loss_per_batch)\n",
    "    \n",
    "    test_loss_per_epoch.append(temp_loss_per_batch/len(test_set_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(test_loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(np.sqrt(test_loss_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per_itr_test_loss_frame = pd.DataFrame({'Itr':range(20), 'MSE_Loss':test_loss_per_epoch})\n",
    "# Code is split across multiple lines for readability and ease of modification\n",
    "# loss_plot = ggplot(per_itr_test_loss_frame, aes(x='Itr', y='MSE_Loss'))\n",
    "# loss_plot = loss_plot + geom_point() + geom_line() + scale_x_continuous(breaks=range(0, 20, 1))\n",
    "# loss_plot = loss_plot + labs(title='MSE Loss Across Iterations (Test Set)', x='Iteration', y='MSE Loss')\n",
    "# loss_plot = loss_plot + theme(plot_title=element_text(face='bold')\n",
    "#                              , axis_title_x=element_text(face='plain', size=12)\n",
    "#                              , axis_title_y=element_text(face='plain', size=12)\n",
    "#                              , figure_size=(15, 5))\n",
    "\n",
    "# loss_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time_2 = datetime.now()\n",
    "time_diff_2 = end_time_2 - start_time\n",
    "time_diff_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time (in minutes)\n",
    "time_diff_2.total_seconds()/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_3_7",
   "language": "python",
   "name": "python_3_7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
